# We provide a mapping from each aten node onto the mlir implementation node.
# We don't need to map every node and nodes which are unmapped may be caught
# by the JIT path which will decompose some operations to onnx.

# Basic format is:
# - func: {ATEN_NODE}
#    PopTorchDirect: {POPTORCH_NODE}

# More attributes can be added as need be. Currently we have:

# PopTorchDirect : FUNC     The mlir implementation of this class with the name
# PopTorchDirectInplace : FUNC   The inplace mlir implementation of this class with the name

# The two above can be used together when the op may or may not be inplace or individually.

# * IgnoreArgs: Ignore these arguments on the schema

# * UnusedOutputArguments: Mark a given input as being unused in the function]
#                        and is instead just used to mark the output. In Aten
#                        this is common as many operations will have an argument
#                        (!out) which is the storage location of the output. If
#                        it matches an input it is inplace.

#

######################
# Activations
######################

- func: hardsigmoid.out
  PopTorchDirect: hardsigmoid
  PopTorchDirectInplace: hardsigmoid_
  UnusedOutputArguments: {out}

- func: hardswish.out
  PopTorchDirect: hardswish
  UnusedOutputArguments: {out}

- func: hardswish
  PopTorchDirect: hardswish

- func: hardswish_
  PopTorchDirectInplace: hardswish_

- func: relu_
  PopTorchDirectInplace: relu_

# tanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
- func: tanh.out
  PopTorchDirect: tanh
  UnusedOutputArguments: {out}


# sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
- func: sigmoid.out
  PopTorchDirect: sigmoid
  UnusedOutputArguments: {out}

- func: silu.out
  PopTorchDirect: swish
  PopTorchDirectInplace: swish_
  UnusedOutputArguments: {out}

# gelu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
- func: gelu.out
  PopTorchDirect: gelu
  UnusedOutputArguments: {out}

# aten::sigmoid_(Tensor(a!) self) -> Tensor(a!)
- func: sigmoid_
  PopTorchDirectInplace: sigmoid_

# _softmax(Tensor self, int dim, bool half_to_float) -> Tensor
- func: _softmax
  PopTorchDirect: softmax

# _softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
- func: _softmax.out
  PopTorchDirect: softmax
  UnusedOutputArguments: out

# _log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
- func: _log_softmax.out
  PopTorchDirect: logsoftmax
  UnusedOutputArguments: out

# _log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
- func: _log_softmax_backward_data.out
  PopTorchDirect: logsoftmax_backward
  UnusedOutputArguments: out


######################
# Views and reshapes
######################

# aten::as_strided(Tensor(a) self, int[] size, int[] stride, int? storage_offset=None) -> (Tensor(a))
- func: as_strided
  PopTorchDirect: as_strided
  IgnoreArgs: {'storage_offset'}

# aten::expand(Tensor(a) self, int[] size, *, bool implicit=False) -> (Tensor(a))
- func: expand
  PopTorchDirect: expand
  IgnoreArgs: {'implicit'}

# aten::reshape(Tensor(a) self, int[] shape) -> (Tensor(a))
# Ideally we would use the native cpu function but have an equivalent
# to the "if (self.is_mkldnn()) {" for ipu tensors. But we can instead
# overwrite and run reshape here.
# NB this may not match CPU implementation but the user is told in the PyTorch
# documentation, "you should not depend on the copying vs. viewing behavior".
- func: reshape
  PopTorchDirect: reshape

# aten::_reshape_alias(Tensor(a) self, int[] size, int[] stride) -> Tensor(a)
# The purpose of this function in PyTorch is to avoid a costly dispatch to view.
# However, we can simply handle it as we do "reshape" and ignore the stride
# (stride is ignored "for now" in as_strided::lowerToPoplar).
- func: _reshape_alias
  PopTorchDirect: reshape
  IgnoreArgs: {'stride'}

- func: transpose.int
  PopTorchDirect: transpose

# aten::view(Tensor(a) self, int[] size) -> (Tensor(a))
# View differs from reshape ordinarily in that reshape may be a view or a copy
# but view must always be a view. Because there is no concept of striding or
# contiguous vs not contiguous tensor in poplar, a rehshape can always be a
# view.
- func: view
  PopTorchDirect: reshape
  
# _cat(Tensor[] tensors, int dim=0) -> Tensor(a!)
- func: _cat
  PopTorchDirect: concat

# squeeze.dim(Tensor(a) self, int dim) -> Tensor(a)
- func: squeeze.dim
  PopTorchDirect: squeeze_dim

# squeeze_.dim(Tensor(a!) self, int dim) -> Tensor(a!)
- func: squeeze_.dim
  PopTorchDirectInplaceReshape: squeeze_dim_
################
# Element wise
################

# aten::mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> (Tensor(a!))
- func: mul.out
  PopTorchDirect: mul
  PopTorchDirectInplace: mul_
  UnusedOutputArguments: {out}

# aten::div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> (Tensor(a!))
- func: div.out
  PopTorchDirect: div
  PopTorchDirectInplace: div_
  UnusedOutputArguments: {out}

# aten::zero_(Tensor(a!) self) -> (Tensor(a!))
- func: zero_
  PopTorchDirectInplace: zero_


# fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)
- func: fill_.Scalar
  PopTorchDirectInplace: fill_


# sub.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
- func: sub.out
  PopTorchDirect: sub
  PopTorchDirectInplace: sub_
  UnusedOutputArguments: {out}


# add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
- func: add.out
  PopTorchDirect: add
  PopTorchDirectInplace: add_
  UnusedOutputArguments: {out}

# aten::addcmul.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> (Tensor(a!))
- func: addcmul.out
  PopTorchDirect: addcmul
  PopTorchDirectInplace: addcmul_
  UnusedOutputArguments: {out}


# aten::addcdiv.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> (Tensor(a!))
- func: addcdiv.out
  PopTorchDirect: addcdiv
  PopTorchDirectInplace: addcdiv_
  UnusedOutputArguments: {out}

# dropout(Tensor input, float p, bool train) -> Tensor
- func: dropout
  PopTorchDirect: dropout

# - func: clamp.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)
- func: clamp.out
  PopTorchDirect: clamp
  UnusedOutputArguments: {out}

# # - func: clamp.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor
- func: clamp.Tensor
  PopTorchDirect: clampTensor

##########
# Linalg
##########


# convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride,
#             int[] padding, int[] dilation, bool transposed,
#             int[] output_padding, int groups) -> Tensor
- func: convolution
  PopTorchDirect: conv
  IgnoreArgs: {'transposed'}


# aten::mm(Tensor self, Tensor mat2) -> (Tensor)
# Matrix-matrix multiplication
- func: mm
  PopTorchDirect: matmul

# aten::mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> (Tensor(a!))
- func: mm.out
  PopTorchDirect: matmul
  UnusedOutputArguments: {out}

# addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
- func: addmm.out
  PopTorchDirect: addmm
  UnusedOutputArguments: {out}


#############
# Random number generation
#############


# `torch.randn`, `torch.randn_like` dispatch to this
# normal_(Tensor(a!) self, float mean=0, float std=1, *, Generator? generator=None) -> Tensor(a!)
- func: normal_
  PopTorchDirect: normal_
  IgnoreArgs: {'generator'}

# normal.Tensor_Tensor(Tensor mean, Tensor std, *, Generator? generator=None) -> Tensor
- func: normal.Tensor_Tensor
  PopTorchDirect: normal_Tensor_Tensor
  IgnoreArgs: {'generator'}
# normal.Tensor_Tensor_out(Tensor mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
- func: normal.Tensor_Tensor_out
  PopTorchDirect: normal_Tensor_Tensor
  IgnoreArgs: {'generator'}
  UnusedOutputArguments: {out}

# normal.Tensor_float(Tensor mean, float std=1, *, Generator? generator=None) -> Tensor
- func: normal.Tensor_float
  PopTorchDirect: normal_Tensor_float
  IgnoreArgs: {'generator'}
# normal.Tensor_float_out(Tensor mean, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
- func: normal.Tensor_float_out
  PopTorchDirect: normal_Tensor_float
  IgnoreArgs: {'generator'}
  UnusedOutputArguments: {out}

# normal.float_Tensor(float mean, Tensor std, *, Generator? generator=None) -> Tensor
- func: normal.float_Tensor
  PopTorchDirect: normal_float_Tensor
  IgnoreArgs: {'generator'}
# normal.float_Tensor_out(float mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
- func: normal.float_Tensor_out
  PopTorchDirect: normal_float_Tensor
  IgnoreArgs: {'generator'}
  UnusedOutputArguments: {out}

# `torch.rand`, `torch.rand_like` dispatch to this
# uniform_(Tensor(a!) self, float from=0, float to=1, *, Generator? generator=None) -> Tensor(a!)
- func: uniform_
  PopTorchDirect: uniform_
  IgnoreArgs: {'generator'}

# `torch.randint`, `torch.randint_like` dispatch to this
# random_(Tensor(a!) self, *, Generator? generator=None) -> Tensor(a!)
- func: random_
  PopTorchDirect: random_
  IgnoreArgs: {'generator'}

# random_.from(Tensor(a!) self, int from, int? to, *, Generator? generator=None) -> Tensor(a!)
- func: random_.from
  PopTorchDirect: random__from
  IgnoreArgs: {'generator'}

# bernoulli_.float(Tensor(a!) self, float p=0.5, *, Generator? generator=None) -> Tensor(a!)
- func: bernoulli_.float
  PopTorchDirect: bernoulli__float
  IgnoreArgs: {'generator'}

# bernoulli_.Tensor(Tensor(a!) self, Tensor p, *, Generator? generator=None) -> Tensor(a!)
- func: bernoulli_.Tensor
  PopTorchDirect: bernoulli__tensor
  IgnoreArgs: {'generator'}

# bernoulli.out(Tensor self, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
- func: bernoulli.out
  PopTorchDirect: bernoulli_out
  IgnoreArgs: {'generator'}
  UnusedOutputArguments: {out}


#########
# Misc
#########

- func: topk.values
  PopTorchDirect: topk
  UnusedOutputArguments: {values, indices}


#########
# Norms
#########


# native_batch_norm(Tensor input, Tensor? weight, Tensor? bias,
#                   Tensor? running_mean, Tensor? running_var,
#                   bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)
- func: native_batch_norm
  PopTorchDirect: batch_norm

# native_group_norm(Tensor input, Tensor? weight, Tensor? bias, int N, int C,
#                   int HxW, int group, float eps) -> (Tensor, Tensor, Tensor)
- func: native_group_norm
  PopTorchDirect: group_norm


#
# Losses
#

# aten::mse_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> (Tensor)
- func: mse_loss_backward
  PopTorchDirect: mse_loss_backward

# nll_loss_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
- func: nll_loss_forward.output
  PopTorchDirect: nll_loss
  UnusedOutputArguments: {output, total_weight}

# nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
- func: nll_loss_backward.grad_input
  PopTorchDirect: nll_loss_backward
  UnusedOutputArguments: {grad_input}


###########
# Pooling
###########

# Note: AvgPool1d also lowers to avg_pool2d.out
# avg_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)
- func: avg_pool2d.out
  PopTorchDirect: avg_pool
  UnusedOutputArguments: {out}

# avg_pool3d.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)
- func: avg_pool3d.out
  PopTorchDirect: avg_pool
  UnusedOutputArguments: {out}

# max_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> Tensor
- func: max_pool1d
  PopTorchDirect: max_pool

# max_pool1d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
- func: max_pool2d
  PopTorchDirect: max_pool

# max_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> Tensor
- func: max_pool3d
  PopTorchDirect: max_pool

# adaptive_avg_pool1d(Tensor self, int[1] output_size) -> Tensor(a!)
- func: adaptive_avg_pool1d
  PopTorchDirect: adaptive_avg_pool

# adaptive_avg_pool2d(Tensor self, int[2] output_size) -> Tensor(a!)
- func: adaptive_avg_pool2d
  PopTorchDirect: adaptive_avg_pool

# adaptive_avg_pool3d(Tensor self, int[3] output_size) -> Tensor(a!)
- func: adaptive_avg_pool3d
  PopTorchDirect: adaptive_avg_pool

# _s_where(Tensor condition, Tensor self, Tensor other) -> Tensor
- func: _s_where
  PopTorchDirect: where
