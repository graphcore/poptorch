# We provide a mapping from each aten node onto the mlir implementation node.
# We don't need to map every node and nodes which are unmapped may be caught
# by the JIT path which will decompose some operations to onnx.

# Basic format is:
# - func: {ATEN_NODE}
#    PopTorchDirect: {POPTORCH_NODE}

# More attributes can be added as need be. Currently we have:

# PopTorchDirect : FUNC     The mlir implementation of this class with the name
# PopTorchDirectInplace : FUNC   The inplace mlir implementation of this class with the name

# The two above can be used together when the op may or may not be inplace or individually.

# * IgnoreArgs: Ignore these arguments on the schema

# * UnusedOutputArguments: Mark a given input as being unused in the function]
#                        and is instead just used to mark the output. In Aten
#                        this is common as many operations will have an argument
#                        (!out) which is the storage location of the output. If
#                        it matches an input it is inplace.

#

######################
# Activations
######################

# hardsigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
- func: hardsigmoid.out
  PopTorchDirect: hardsigmoid
  PopTorchDirectInplace: hardsigmoid_
  UnusedOutputArguments: {out}

# hardswish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
- func: hardswish.out
  PopTorchDirect: hardswish
  UnusedOutputArguments: {out}

# hardswish(Tensor self) -> Tensor
- func: hardswish
  PopTorchDirect: hardswish

# func: hardshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
- func: hardshrink.out
  PopTorchDirect: hardshrink
  UnusedOutputArguments: {out}

# func: softshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
- func: softshrink.out
  PopTorchDirect: softshrink_out
  UnusedOutputArguments: {out}

# hardswish_(Tensor(a!) self) -> Tensor(a!)
- func: hardswish_
  PopTorchDirectInplace: hardswish_

# relu_(Tensor(a!) self) -> Tensor(a!)
- func: relu_
  PopTorchDirectInplace: relu_

# relu(Tensor self) -> Tensor
- func: relu
  PopTorchDirect: relu

# func: rrelu_with_noise(Tensor self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor
- func: rrelu_with_noise
  PopTorchDirect: rrelu_with_noise
  IgnoreArgs: {'generator'}

# leaky_relu.out(Tensor self, Scalar negative_slope=0.01, *, Tensor(a!) out) -> Tensor(a!)
- func: leaky_relu.out
  PopTorchDirect: leaky_relu
  UnusedOutputArguments: {out}

# leaky_relu_backward.grad_input(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result, *, Tensor(a!) grad_input) -> Tensor(a!)
- func: leaky_relu_backward.grad_input
  PopTorchDirect: leaky_relu_backward
  UnusedOutputArguments: {grad_input}

# func: prelu(Tensor self, Tensor weight) -> Tensor
- func: prelu
  PopTorchDirect: prelu
  UnusedOutputArguments: {out}

# tanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
- func: tanh.out
  PopTorchDirect: tanh
  UnusedOutputArguments: {out}

# tanh_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
- func: tanh_backward.grad_input
  PopTorchDirect: tanh_backward
  UnusedOutputArguments: {grad_input}

# func: softplus.out(Tensor self, Scalar beta=1, Scalar threshold=20, *, Tensor(a!) out) -> Tensor(a!)
- func: softplus.out
  PopTorchDirect: softplus
  UnusedOutputArguments: {out}

# sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
- func: sigmoid.out
  PopTorchDirect: sigmoid
  UnusedOutputArguments: {out}

# sigmoid_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
- func: sigmoid_backward.grad_input
  PopTorchDirect: sigmoid_backward
  UnusedOutputArguments: {grad_input}

- func: silu.out
  PopTorchDirect: swish
  PopTorchDirectInplace: swish_
  UnusedOutputArguments: {out}

# gelu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
- func: gelu.out
  PopTorchDirect: gelu
  UnusedOutputArguments: {out}

# elu.out(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1, *, Tensor(a!) out) -> Tensor(a!)
- func: elu.out
  PopTorchDirect: elu
  UnusedOutputArguments: {out}

# aten::sigmoid_(Tensor(a!) self) -> Tensor(a!)
- func: sigmoid_
  PopTorchDirectInplace: sigmoid_

# _softmax(Tensor self, int dim, bool half_to_float) -> Tensor
- func: _softmax
  PopTorchDirect: softmax

# _softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
- func: _softmax.out
  PopTorchDirect: softmax
  UnusedOutputArguments: out

# _log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
- func: _log_softmax.out
  PopTorchDirect: logsoftmax
  UnusedOutputArguments: out

# _log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
- func: _log_softmax_backward_data.out
  PopTorchDirect: logsoftmax_backward
  UnusedOutputArguments: out

# func: log_sigmoid_forward(Tensor self) -> (Tensor output, Tensor buffer)
- func: log_sigmoid_forward
  PopTorchDirect: log_sigmoid_forward

# func: glu.out(Tensor self, int dim=-1, *, Tensor(a!) out) -> Tensor(a!)
- func: glu.out
  PopTorchDirect: glu_out
  UnusedOutputArguments: {out}

######################
# Views and reshapes
######################

# aten::as_strided(Tensor(a) self, int[] size, int[] stride, int? storage_offset=None) -> (Tensor(a))
- func: as_strided
  PopTorchDirect: as_strided

# aten::expand(Tensor(a) self, int[] size, *, bool implicit=False) -> (Tensor(a))
- func: expand
  PopTorchDirect: expand

# aten::reshape(Tensor(a) self, int[] shape) -> (Tensor(a))
# Ideally we would use the native cpu function but have an equivalent
# to the "if (self.is_mkldnn()) {" for ipu tensors. But we can instead
# overwrite and run reshape here.
# NB this may not match CPU implementation but the user is told in the PyTorch
# documentation, "you should not depend on the copying vs. viewing behavior".
- func: reshape
  PopTorchDirect: reshape

# aten::_reshape_alias(Tensor(a) self, int[] size, int[] stride) -> Tensor(a)
# The purpose of this function in PyTorch is to avoid a costly dispatch to view.
# However, we can simply handle it as we do "reshape" and ignore the stride
# (stride is ignored "for now" in as_strided::lowerToPoplar).
- func: _reshape_alias
  PopTorchDirect: reshape
  IgnoreArgs: {'stride'}

- func: transpose.int
  PopTorchDirect: transpose

# aten::view(Tensor(a) self, int[] size) -> (Tensor(a))
# View differs from reshape ordinarily in that reshape may be a view or a copy
# but view must always be a view. Because there is no concept of striding or
# contiguous vs not contiguous tensor in poplar, a rehshape can always be a
# view.
- func: view
  PopTorchDirect: reshape

# _cat(Tensor[] tensors, int dim=0) -> Tensor(a!)
- func: _cat
  PopTorchDirect: concat

# squeeze.dim(Tensor(a) self, int dim) -> Tensor(a)
- func: squeeze.dim
  PopTorchDirect: squeeze_dim

# squeeze_.dim(Tensor(a!) self, int dim) -> Tensor(a!)
- func: squeeze_.dim
  PopTorchDirectInplaceReshape: squeeze_dim_

# select.int(Tensor(a) self, int dim, int index) -> Tensor(a)
- func: select.int
  PopTorchDirect: select

# func: slice.Tensor(Tensor(a) self, int dim=0, int? start=None, int? end=None, int step=1) -> Tensor(a)
- func: slice.Tensor
  PopTorchDirect: slice_Tensor

# unsqueeze(Tensor(a) self, int dim) -> Tensor(a)
- func: unsqueeze
  PopTorchDirect: unsqueeze

# func: constant_pad_nd(Tensor self, int[] pad, Scalar value=0) -> Tensor
- func: constant_pad_nd
  PopTorchDirect: constant_pad_nd

# func: reflection_pad1d.out(Tensor self, int[2] padding, *, Tensor(a!) out) -> Tensor(a!)
- func: reflection_pad1d.out
  PopTorchDirect: reflection_pad1d_out
  UnusedOutputArguments: {out}

# func: reflection_pad2d(Tensor self, int[4] padding) -> Tensor
- func: reflection_pad2d
  PopTorchDirect: reflection_pad2d

# func: replication_pad1d.out(Tensor self, int[2] padding, *, Tensor(a!) out) -> Tensor(a!)
- func: replication_pad1d.out
  PopTorchDirect: replication_pad1d_out
  UnusedOutputArguments: {out}

# func: replication_pad2d.out(Tensor self, int[4] padding, *, Tensor(a!) out) -> Tensor(a!)
- func: replication_pad2d.out
  PopTorchDirect: replication_pad2d_out
  UnusedOutputArguments: {out}

# func: replication_pad3d.out(Tensor self, int[6] padding, *, Tensor(a!) out) -> Tensor(a!)
- func: replication_pad3d.out
  PopTorchDirect: replication_pad3d_out
  UnusedOutputArguments: {out}

# func: index_select(Tensor self, int dim, Tensor index) -> Tensor
- func: index_select
  PopTorchDirect: index_select

# func: permute(Tensor(a) self, int[] dims) -> Tensor(a)
- func: permute
  PopTorchDirect: permute

# func: upsample_trilinear3d.out(Tensor self, int[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
- func: upsample_trilinear3d.out
  PopTorchDirect: upsample_trilinear3d_out
  UnusedOutputArguments: {out}

# func: upsample_bilinear2d.out(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
- func: upsample_bilinear2d.out
  PopTorchDirect: upsample_bilinear2d_out
  UnusedOutputArguments: {out}

# func: upsample_nearest1d.out(Tensor self, int[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
- func: upsample_nearest1d.out
  PopTorchDirect: upsample_nearest1d_out
  UnusedOutputArguments: {out}

# func: upsample_nearest2d.out(Tensor self, int[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
- func: upsample_nearest2d.out
  PopTorchDirect: upsample_nearest2d_out
  UnusedOutputArguments: {out}

# func: upsample_nearest3d.out(Tensor self, int[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
- func: upsample_nearest3d.out
  PopTorchDirect: upsample_nearest3d_out
  UnusedOutputArguments: {out}

# func: upsample_nearest3d.vec(Tensor input, int[]? output_size, float[]? scale_factors) -> Tensor
# This isn't being forwarded to 'upsample_nearest3d.out' so we need to do our own computation of the output_size
- func: upsample_nearest3d.vec
  PopTorchDirect: upsample_nearest3d_vec
  UnusedOutputArguments: {out}

# func: upsample_bicubic2d.out(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
- func: upsample_bicubic2d.out
  PopTorchDirect: upsample_bicubic2d_out
  UnusedOutputArguments: {out}

# func: upsample_linear1d.out(Tensor self, int[1] output_size, bool align_corners, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
- func: upsample_linear1d.out
  PopTorchDirect: upsample_linear1d_out
  UnusedOutputArguments: {out}

################
# Element wise
################

# aten::mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> (Tensor(a!))
- func: mul.out
  PopTorchDirect: mul
  PopTorchDirectInplace: mul_
  UnusedOutputArguments: {out}

# aten::div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> (Tensor(a!))
- func: div.out
  PopTorchDirect: div
  PopTorchDirectInplace: div_
  UnusedOutputArguments: {out}

# func: remainder.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
- func: remainder.Tensor_out
  PopTorchDirect: remainder_Tensor_out
  UnusedOutputArguments: {out}

# func: round.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
- func: round.out
  PopTorchDirect: round_out
  UnusedOutputArguments: {out}

# func: fmod.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
- func: fmod.Tensor_out
  PopTorchDirect: fmod
  UnusedOutputArguments: {out}

# func: floor_divide(Tensor self, Tensor other) -> Tensor
- func: floor_divide
  PopTorchDirect: floor_divide

# func: sign.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
- func: sign.out
  PopTorchDirect: signum
  PopTorchDirectInplace: signum_
  UnusedOutputArguments: {out}

# aten::zero_(Tensor(a!) self) -> (Tensor(a!))
- func: zero_
  PopTorchDirectInplace: zero_


# fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)
- func: fill_.Scalar
  PopTorchDirectInplace: fill_


# sub.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
- func: sub.out
  PopTorchDirect: sub
  PopTorchDirectInplace: sub_
  UnusedOutputArguments: {out}


# add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
- func: add.out
  PopTorchDirect: add
  PopTorchDirectInplace: add_
  UnusedOutputArguments: {out}

# aten::addcmul.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> (Tensor(a!))
- func: addcmul.out
  PopTorchDirect: addcmul
  PopTorchDirectInplace: addcmul_
  UnusedOutputArguments: {out}


# aten::addcdiv.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> (Tensor(a!))
- func: addcdiv.out
  PopTorchDirect: addcdiv
  PopTorchDirectInplace: addcdiv_
  UnusedOutputArguments: {out}

# func: exp.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
- func: exp.out
  PopTorchDirect: exp
  UnusedOutputArguments: {out}

# func: log.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
- func: log.out
  PopTorchDirect: log
  UnusedOutputArguments: {out}

# func: pow.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)
- func: pow.Tensor_Scalar_out
  PopTorchDirect: pow_Tensor_Scalar_out
  UnusedOutputArguments: {out}

# func: bitwise_and.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
- func: bitwise_and.Tensor_out
  PopTorchDirect: bitwiseAnd
  PopTorchDirectInplace: bitwiseAnd_
  UnusedOutputArguments: {out}

# func: bitwise_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
- func: bitwise_not.out
  PopTorchDirect: bitwiseNot
  PopTorchDirectInplace: bitwiseNot_
  UnusedOutputArguments: {out}

# func: bitwise_or.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
- func: bitwise_or.Tensor_out
  PopTorchDirect: bitwiseOr
  PopTorchDirectInplace: bitwiseOr_
  UnusedOutputArguments: {out}

# func: bitwise_xor.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
- func: bitwise_xor.Tensor_out
  PopTorchDirect: bitwiseXor
  PopTorchDirectInplace: bitwiseXor_
  UnusedOutputArguments: {out}

# func: logical_and.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
- func: logical_and.out
  PopTorchDirect: logicalAnd
  PopTorchDirectInplace: logicalAnd_
  UnusedOutputArguments: {out}

# func: logical_or.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
- func: logical_or.out
  PopTorchDirect: logicalOr
  PopTorchDirectInplace: logicalOr_
  UnusedOutputArguments: {out}

# func: logical_xor.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
- func: logical_xor.out
  PopTorchDirect: logicalXor
  UnusedOutputArguments: {out}

# func: logical_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
- func: logical_not.out
  PopTorchDirect: logicalNot
  PopTorchDirectInplace: logicalNot_
  UnusedOutputArguments: {out}

# abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
- func: abs.out
  PopTorchDirect: abs
  PopTorchDirectInplace: abs_
  UnusedOutputArguments: {out}

# neg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
- func: neg.out
  PopTorchDirect: neg
  PopTorchDirectInplace: neg_
  UnusedOutputArguments: {out}

# func: minimum(Tensor self, Tensor other) -> Tensor
- func: minimum
  PopTorchDirect: min
  PopTorchDirectInplace: min_

# func: maximum(Tensor self, Tensor other) -> Tensor
- func: maximum
  PopTorchDirect: max
  PopTorchDirectInplace: max_

# clamp.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)
- func: clamp.out
  PopTorchDirect: clamp
  UnusedOutputArguments: {out}

# clamp.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor
- func: clamp.Tensor
  PopTorchDirect: clampTensor

# func: ceil.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
- func: ceil.out
  PopTorchDirect: ceil
  PopTorchDirectInplace: ceil_
  UnusedOutputArguments: {out}

# func: floor.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
- func: floor.out
  PopTorchDirect: floor
  PopTorchDirectInplace: floor_
  UnusedOutputArguments: {out}

# func: trunc(Tensor self) -> Tensor
- func: trunc
  PopTorchDirect: trunc
  UnusedOutputArguments: {out}

# func: threshold.out(Tensor self, Scalar threshold, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
- func: threshold.out
  PopTorchDirect: threshold_out
  UnusedOutputArguments: {out}

# hardtanh.out(Tensor self, Scalar min_val=-1, Scalar max_val=1, *, Tensor(a!) out) -> Tensor(a!)
- func: hardtanh.out
  PopTorchDirect: clamp
  UnusedOutputArguments: {out}

# hardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1) -> Tensor
- func: hardtanh
  PopTorchDirect: clamp

# func: isnan(Tensor self) -> Tensor
- func: isnan
  PopTorchDirect: isnan

# gt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
- func: gt.Tensor_out
  PopTorchDirect: gt
  PopTorchDirectInplace: gt_
  UnusedOutputArguments: {out}

# ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
- func: ge.Tensor_out
  PopTorchDirect: gteq
  PopTorchDirectInplace: gteq_
  UnusedOutputArguments: {out}

# lt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
- func: lt.Tensor_out
  PopTorchDirect: lt
  PopTorchDirectInplace: lt_
  UnusedOutputArguments: {out}

# le.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
- func: le.Tensor_out
  PopTorchDirect: lteq
  PopTorchDirectInplace: lteq_
  UnusedOutputArguments: {out}

# func: eq.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
- func: eq.Tensor_out
  PopTorchDirect: eq
  PopTorchDirectInplace: eq_
  UnusedOutputArguments: {out}

# func: ne.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
- func: ne.Tensor_out
  PopTorchDirect: neq
  PopTorchDirectInplace: neq_
  UnusedOutputArguments: {out}

# func: lt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
- func: lt.Scalar_out
  PopTorchDirect: lt_Scalar_out
  UnusedOutputArguments: {out}

# func: less_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
- func: le.Scalar_out
  PopTorchDirect: le_Scalar_out
  UnusedOutputArguments: {out}

# func: gt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
- func: gt.Scalar_out
  PopTorchDirect: gt_Scalar_out
  UnusedOutputArguments: {out}

# func: greater_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
- func: ge.Scalar_out
  PopTorchDirect: ge_Scalar_out
  UnusedOutputArguments: {out}

# func: eq.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
- func: eq.Scalar_out
  PopTorchDirect: eq_Scalar_out
  UnusedOutputArguments: {out}

# func: eq.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
- func: ne.Scalar_out
  PopTorchDirect: ne_Scalar_out
  UnusedOutputArguments: {out}

# sqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
- func: sqrt.out
  PopTorchDirect: sqrt
  UnusedOutputArguments: {out}

# func: scatter_add.out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)
- func: scatter_add.out
  PopTorchDirect: scatter_add_out
  UnusedOutputArguments: {out}

# dropout(Tensor input, float p, bool train) -> Tensor
- func: dropout
  PopTorchDirect: dropout


##########
# Linalg
##########

# convolution_overrideable(Tensor input, Tensor weight, Tensor? bias,
#                          int[] stride, int[] padding, int[] dilation,
#                          bool transposed, int[] output_padding,
#                          int groups) -> Tensor
- func: convolution_overrideable
  PopTorchDirect: conv

# aten::convolution_backward_overrideable(Tensor grad_output, Tensor input,
#                                         Tensor weight, int[] stride,
#                                         int[] padding, int[] dilation,
#                                         bool transposed, int[] output_padding,
#                                         int groups, bool[3] output_mask) ->
#                                         (Tensor grad_input, Tensor grad_weight,
#                                          Tensor grad_bias)
- func: convolution_backward_overrideable
  PopTorchDirect: conv_backward

# bmm(Tensor self, Tensor mat2) -> (Tensor)
# Batch matrix-matrix multiplication
- func: bmm
  PopTorchDirect: matmul

# func: bmm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
# Batch matrix-matrix multiplication
- func: bmm.out
  PopTorchDirect: matmul
  UnusedOutputArguments: {out}

# mm(Tensor self, Tensor mat2) -> (Tensor)
# Matrix-matrix multiplication
- func: mm
  PopTorchDirect: matmul

# mv(Tensor self, Tensor vec) -> Tensor
# Matrix-vector multiplication
- func: mv
  PopTorchDirect: matmul

# dot(Tensor self, Tensor tensor) -> Tensor
- func: dot
  PopTorchDirect: matmul

# mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> (Tensor(a!))
- func: mm.out
  PopTorchDirect: matmul
  UnusedOutputArguments: {out}

# addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
- func: addmm.out
  PopTorchDirect: addmm
  UnusedOutputArguments: {out}

# func: norm.out(Tensor self, Scalar? p, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
- func: norm.out
  PopTorchDirect: norm_out
  UnusedOutputArguments: {out}

#############
# Random number generation
#############


# `torch.randn`, `torch.randn_like` dispatch to this
# normal_(Tensor(a!) self, float mean=0, float std=1, *, Generator? generator=None) -> Tensor(a!)
- func: normal_
  PopTorchDirect: normal_
  IgnoreArgs: {'generator'}

# normal.Tensor_Tensor(Tensor mean, Tensor std, *, Generator? generator=None) -> Tensor
- func: normal.Tensor_Tensor
  PopTorchDirect: normal_Tensor_Tensor
  IgnoreArgs: {'generator'}
# normal.Tensor_Tensor_out(Tensor mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
- func: normal.Tensor_Tensor_out
  PopTorchDirect: normal_Tensor_Tensor
  IgnoreArgs: {'generator'}
  UnusedOutputArguments: {out}

# normal.Tensor_float(Tensor mean, float std=1, *, Generator? generator=None) -> Tensor
- func: normal.Tensor_float
  PopTorchDirect: normal_Tensor_float
  IgnoreArgs: {'generator'}
# normal.Tensor_float_out(Tensor mean, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
- func: normal.Tensor_float_out
  PopTorchDirect: normal_Tensor_float
  IgnoreArgs: {'generator'}
  UnusedOutputArguments: {out}

# normal.float_Tensor(float mean, Tensor std, *, Generator? generator=None) -> Tensor
- func: normal.float_Tensor
  PopTorchDirect: normal_float_Tensor
  IgnoreArgs: {'generator'}
# normal.float_Tensor_out(float mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
- func: normal.float_Tensor_out
  PopTorchDirect: normal_float_Tensor
  IgnoreArgs: {'generator'}
  UnusedOutputArguments: {out}

# `torch.rand`, `torch.rand_like` dispatch to this
# uniform_(Tensor(a!) self, float from=0, float to=1, *, Generator? generator=None) -> Tensor(a!)
- func: uniform_
  PopTorchDirect: uniform_
  IgnoreArgs: {'generator'}

# `torch.randint`, `torch.randint_like` dispatch to this
# random_(Tensor(a!) self, *, Generator? generator=None) -> Tensor(a!)
- func: random_
  PopTorchDirect: random_
  IgnoreArgs: {'generator'}

# random_.from(Tensor(a!) self, int from, int? to, *, Generator? generator=None) -> Tensor(a!)
- func: random_.from
  PopTorchDirect: random__from
  IgnoreArgs: {'generator'}

# func: exponential_(Tensor(a!) self, float lambd=1, *, Generator? generator=None) -> Tensor(a!)
- func: exponential_
  PopTorchDirect: exponential_
  IgnoreArgs: {'generator'}

# bernoulli_.float(Tensor(a!) self, float p=0.5, *, Generator? generator=None) -> Tensor(a!)
- func: bernoulli_.float
  PopTorchDirect: bernoulli__float
  IgnoreArgs: {'generator'}

# bernoulli_.Tensor(Tensor(a!) self, Tensor p, *, Generator? generator=None) -> Tensor(a!)
- func: bernoulli_.Tensor
  PopTorchDirect: bernoulli__tensor
  IgnoreArgs: {'generator'}

# bernoulli.out(Tensor self, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
- func: bernoulli.out
  PopTorchDirect: bernoulli_out
  IgnoreArgs: {'generator'}
  UnusedOutputArguments: {out}

# randperm.generator_out(int n, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
- func: randperm.generator_out
  PopTorchDirect: randperm
  IgnoreArgs: {'generator'}
  UnusedOutputArguments: {out}


#########
# Misc
#########

# to.dtype(Tensor(a) self, ScalarType dtype, bool non_blocking=False,
#          bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
- func: to.dtype
  PopTorchDirect: cast
  IgnoreArgs: non_blocking, copy, memory_format

# func: arange.start_out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)
- func: arange.start_out
  PopTorchDirect: arange_start_out
  UnusedOutputArguments: {out}

- func: topk.values
  PopTorchDirect: topk
  UnusedOutputArguments: {values, indices}

# sum.IntList_out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
- func: sum.IntList_out
  PopTorchDirect: reducesum
  UnusedOutputArguments: {out}

# func: cumsum.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
- func: cumsum.out
  PopTorchDirect: cumsum_out
  UnusedOutputArguments: {out}

# mean.out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
- func: mean.out
  PopTorchDirect: reducemean
  UnusedOutputArguments: {out}

# func: all(Tensor self) -> Tensor
- func: all
  PopTorchDirect: all

# func: any(Tensor self) -> Tensor
- func: any
  PopTorchDirect: any

# func: all.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
- func: all.out
  PopTorchDirect: all_out
  UnusedOutputArguments: {out}

# func: any.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
- func: any.out
  PopTorchDirect: any_out
  UnusedOutputArguments: {out}

# func: prod(Tensor self, *, ScalarType? dtype=None) -> Tensor
- func: prod
  PopTorchDirect: prod

# func: prod.int_out(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
- func: prod.int_out
  PopTorchDirect: prod_dim
  UnusedOutputArguments: {out}

# func: argmax.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
- func: argmax.out
  PopTorchDirect: argmax_out
  UnusedOutputArguments: {out}

# func: argmin.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
- func: argmin.out
  PopTorchDirect: argmin_out
  UnusedOutputArguments: {out}

# func: argsort(Tensor self, int dim=-1, bool descending=False) -> Tensor
- func: argsort
  PopTorchDirect: argsort

# func: std.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> Tensor
- func: std.correction
  PopTorchDirect: std_correction

# func: std_mean.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> (Tensor, Tensor)
- func: std_mean.correction
  PopTorchDirect: std_mean_correction

# func: var.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> Tensor
- func: var.correction
  PopTorchDirect: var_correction

# func: var_mean.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> (Tensor, Tensor)
- func: var_mean.correction
  PopTorchDirect: var_mean_correction

# func: zeros_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
- func: zeros_like
  PopTorchDirect: zeros_like
  IgnoreArgs: {'dtype', 'layout', 'device', 'pin_memory', 'memory_format'}

# func: full_like(Tensor self, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
- func: full_like
  PopTorchDirect: full_like
  IgnoreArgs: {'dtype', 'layout', 'device', 'pin_memory', 'memory_format'}

# func: one_hot(Tensor self, int num_classes=-1) -> Tensor
- func: one_hot
  PopTorchDirect: one_hot

# func: masked_fill.Scalar(Tensor self, Tensor mask, Scalar value) -> Tensor
- func: masked_fill.Scalar
  PopTorchDirect: masked_fill_Scalar

# func: _embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)
- func: _embedding_bag
  PopTorchDirect: embedding_bag

#########
# Norms
#########


# native_batch_norm(Tensor input, Tensor? weight, Tensor? bias,
#                   Tensor? running_mean, Tensor? running_var,
#                   bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)
- func: native_batch_norm
  PopTorchDirect: batch_norm

# aten::native_batch_norm_backward(Tensor grad_out, Tensor input,
#                                  Tensor? weight, Tensor? running_mean,
#                                  Tensor? running_var, Tensor? save_mean,
#                                  Tensor? save_invstd, bool train, float eps,
#                                  bool[3] output_mask) -> (Tensor, Tensor, Tensor)
- func: native_batch_norm_backward
  PopTorchDirect: batch_norm_backward

# native_group_norm(Tensor input, Tensor? weight, Tensor? bias, int N, int C,
#                   int HxW, int group, float eps) -> (Tensor, Tensor, Tensor)
- func: native_group_norm
  PopTorchDirect: group_norm

# aten::native_group_norm_backward(Tensor grad_out, Tensor input, Tensor mean,
#                                  Tensor rstd, Tensor? weight, int N, int C,
#                                  int HxW, int group, bool[3] output_mask)
#                                  -> (Tensor, Tensor, Tensor)
- func: native_group_norm_backward
  PopTorchDirect: group_norm_backward


# aten::native_layer_norm(Tensor input, int[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -> (Tensor, Tensor, Tensor)
- func: native_layer_norm
  PopTorchDirect: layer_norm

# aten::native_layer_norm_backward(Tensor grad_out, Tensor input, int[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
- func: native_layer_norm_backward
  PopTorchDirect: layer_norm_backward


#
# Losses
#

# mse_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
- func: mse_loss
  PopTorchDirect: mse_loss

# aten::mse_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> (Tensor)
- func: mse_loss_backward
  PopTorchDirect: mse_loss_backward

# nll_loss_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
- func: nll_loss_forward.output
  PopTorchDirect: nll_loss
  UnusedOutputArguments: {output, total_weight}

# nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
- func: nll_loss_backward.grad_input
  PopTorchDirect: nll_loss_backward
  UnusedOutputArguments: {grad_input}

# binary_cross_entropy.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor(a!)
- func: binary_cross_entropy
  PopTorchDirect: binary_cross_entropy

# binary_cross_entropy_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)
- func: binary_cross_entropy_backward
  PopTorchDirect: binary_cross_entropy_backward

# binary_cross_entropy_with_logits(Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean) -> Tensor
- func: binary_cross_entropy_with_logits
  PopTorchDirect: binary_cross_entropy_with_logits

# binary_cross_entropy_with_logits_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean) -> Tensor
- func: binary_cross_entropy_with_logits_backward
  PopTorchDirect: binary_cross_entropy_with_logits_backward

# func: smooth_l1_loss(Tensor self, Tensor target, int reduction=Mean, float beta=1.0) -> Tensor
- func: smooth_l1_loss
  PopTorchDirect: smooth_l1_loss

###########
# Pooling
###########

# Note: AvgPool1d also lowers to avg_pool2d.out
# avg_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)
- func: avg_pool2d.out
  PopTorchDirect: avg_pool
  UnusedOutputArguments: {out}

# avg_pool3d.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)
- func: avg_pool3d.out
  PopTorchDirect: avg_pool
  UnusedOutputArguments: {out}

# max_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> Tensor
- func: max_pool1d
  PopTorchDirect: max_pool

# max_pool1d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
- func: max_pool2d
  PopTorchDirect: max_pool

# max_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> Tensor
- func: max_pool3d
  PopTorchDirect: max_pool

# adaptive_avg_pool1d(Tensor self, int[1] output_size) -> Tensor(a!)
- func: adaptive_avg_pool1d
  PopTorchDirect: adaptive_avg_pool

# adaptive_avg_pool2d(Tensor self, int[2] output_size) -> Tensor(a!)
- func: adaptive_avg_pool2d
  PopTorchDirect: adaptive_avg_pool

# adaptive_avg_pool3d(Tensor self, int[3] output_size) -> Tensor(a!)
- func: adaptive_avg_pool3d
  PopTorchDirect: adaptive_avg_pool

# _s_where(Tensor condition, Tensor self, Tensor other) -> Tensor
- func: _s_where
  PopTorchDirect: where
